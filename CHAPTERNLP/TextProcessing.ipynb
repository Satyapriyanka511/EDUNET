{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5824bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "#without using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7638221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'iam', 'priyanka,', 'taking', 'AICW', 'session', 'at', 'KIET,from', 'EDUNET,', 'it', 'is', 'a', 'microsoft', 'program']\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"Hello iam priyanka,\n",
    "        taking AICW session at KIET,from EDUNET,\n",
    "        it is a microsoft program\"\"\"\n",
    "\n",
    "word_token=text.split()\n",
    "print(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88b4572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello iam priyanka', '\\n        taking AICW session at KIET', 'from EDUNET', '\\n        it is a microsoft program']\n"
     ]
    }
   ],
   "source": [
    "sent_token=text.split(',')\n",
    "print(sent_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acda05bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization using nltk\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b91eb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'iam', 'priyanka', ',', 'taking', 'AICW', 'session', 'at', 'KIET', ',', 'from', 'EDUNET', ',', 'it', 'is', 'a', 'microsoft', 'program']\n"
     ]
    }
   ],
   "source": [
    "#word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text=\"\"\"Hello iam priyanka,\n",
    "        taking AICW session at KIET,from EDUNET,\n",
    "        it is a microsoft program\"\"\"\n",
    "\n",
    "word_tokens=nltk.word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee4e3ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello iam priyanka,\\n        taking AICW session at KIET,from EDUNET,\\n        it is a microsoft program']\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"\"\"Hello iam priyanka,\n",
    "        taking AICW session at KIET,from EDUNET,\n",
    "        it is a microsoft program\"\"\"\n",
    "sent_tokens=nltk.sent_tokenize(text)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b95aa596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Error downloading 'stopwords' from\n",
      "[nltk_data]     <https://raw.githubusercontent.com/nltk/nltk_data/gh-\n",
      "[nltk_data]     pages/packages/corpora/stopwords.zip>:   <urlopen\n",
      "[nltk_data]     error [Errno 11001] getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "#stopword removal\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text=\"\"\"my name is priyanka from kothapeta we are going\n",
    "to conduct session in KIET it is very important for\n",
    "AIML students\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14491fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#stopword removal\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words('english')) #assign stop words\n",
    "\n",
    "text=\"\"\"my name is priyanka from kothapeta we are going\n",
    "to conduct session in KIET it is very important for\n",
    "AIML students\"\"\"\n",
    "\n",
    "#step 1 convert text into in token\n",
    "word_token=nltk.word_tokenize(text)\n",
    "\n",
    "#step 2 remove the stopwards\n",
    "\n",
    "after_remove_text=[] #empty string\n",
    "\n",
    "for w in word_token:\n",
    "    if w not in stop_words:\n",
    "        after_remove_text.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is priyanka from kothapeta we are going\n",
      "to conduct session in KIET it is very important for\n",
      "AIML students\n",
      "-----------\n",
      "['name', 'priyanka', 'kothapeta', 'going', 'conduct', 'session', 'KIET', 'important', 'AIML', 'students']\n"
     ]
    }
   ],
   "source": [
    "print (text)\n",
    "print(\"----------------------------------\")\n",
    "print (after_remove_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c30e4789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Priya\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priya : NN\n",
      "running : VBG\n",
      "on : IN\n",
      "a : DT\n",
      "track : NN\n",
      "of : IN\n",
      "KIET : NNP\n",
      "college : NN\n",
      ", : ,\n",
      "that : WDT\n",
      "was : VBD\n",
      "a : DT\n",
      "very : RB\n",
      "good : JJ\n",
      "experience : NN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "#part of speech=tagging\n",
    "#priyanka-noun\n",
    "#running-verb\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('punkt') #tokens\n",
    "nltk.download('averaged_perceptron_tagger_eng') #pos\n",
    "text=\"\"\"priya running on a track of KIET college,\n",
    "        that was a very good experience\"\"\"\n",
    "\n",
    "#tokenization\n",
    "word_tokens=nltk.word_tokenize(text)\n",
    "\n",
    "#POS tagging\n",
    "pos_tags = pos_tag(word_tokens)\n",
    "\n",
    "#display tags\n",
    "for w , pos_tag in pos_tags:\n",
    "    print(w,\":\",pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of word\n",
    "#this is a document\n",
    "#this is a second document\n",
    "#and this is third\n",
    "#so it is first\n",
    "#then who is last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4dfb0492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>last</th>\n",
       "      <th>second</th>\n",
       "      <th>so</th>\n",
       "      <th>then</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>who</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>this is a document</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this is a second document</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and this is third</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so it is first</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then who is last</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           and  document  first  is  it  last  second  so  \\\n",
       "this is a document           0         1      0   1   0     0       0   0   \n",
       "this is a second document    0         1      0   1   0     0       1   0   \n",
       "and this is third            1         0      0   1   0     0       0   0   \n",
       "so it is first               0         0      1   1   1     0       0   1   \n",
       "then who is last             0         0      0   1   0     1       0   0   \n",
       "\n",
       "                           then  third  this  who  \n",
       "this is a document            0      0     1    0  \n",
       "this is a second document     0      0     1    0  \n",
       "and this is third             0      1     1    0  \n",
       "so it is first                0      0     0    0  \n",
       "then who is last              1      0     0    1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "text=['this is a document',\n",
    "      'this is a second document',\n",
    "      'and this is third',\n",
    "      'so it is first',\n",
    "      'then who is last']\n",
    "vectorizer=CountVectorizer()\n",
    "x=vectorizer.fit_transform(text)\n",
    "columns=vectorizer.get_feature_names_out()\n",
    "\n",
    "df=pd.DataFrame(x.toarray(),columns=columns,index=text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07e0e67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'waint', 'univers', 'eat']\n"
     ]
    }
   ],
   "source": [
    "#stem\n",
    "from nltk.stem import PorterStemmer\n",
    "port_stemmer=PorterStemmer()\n",
    "words=[\"running\",\"wainting\",\"university\",\"eating\"]\n",
    "stem_words=[port_stemmer.stem(word) for word in words]\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "476161dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'waint', 'univers', 'eat']\n"
     ]
    }
   ],
   "source": [
    "#stem\n",
    "from nltk.stem import SnowballStemmer\n",
    "snow_stemmer=SnowballStemmer('english')\n",
    "words=[\"running\",\"wainting\",\"university\",\"eating\"]\n",
    "stem_words=[snow_stemmer.stem(word) for word in words]\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e114f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corr', 'canin', 'universidd', 'com']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow_stemmer=SnowballStemmer(language='spanish')\n",
    "words=[\"corriendo\",\"caninando\",\"universiddad\",\"comiendo\"]\n",
    "stem_words=[snow_stemmer.stem(word) for word in words]\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a437285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Error loading average_perception_tagger_eng: Package\n",
      "[nltk_data]     'average_perception_tagger_eng' not found in index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orizal words: ['the', 'dogs', 'are', 'running', 'towards', 'cats', 'and', 'catches', 'the', 'all', 'horses']\n"
     ]
    }
   ],
   "source": [
    "#limitization \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('average_perception_tagger_eng')\n",
    "\n",
    "text=\"the dogs are running towards cats and catches the all horses\"\n",
    "limitizer=WordNetLemmatizer()\n",
    "word_tokens=nltk.word_tokenize(text)\n",
    "\n",
    "limitizer_words=[limitizer.lemmatize(word) for word in word_tokens]\n",
    "\n",
    "print(\"orizal words:\",word_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
